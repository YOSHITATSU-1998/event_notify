# scrapers/congress_b.py
# ç¦å²¡å›½éš›ä¼šè­°å ´ï¼ˆæ€ã„å‡ºãƒãƒ¼ãƒ : ã‚³ãƒ³ã‚°ãƒ¬ã‚¹Bï¼‰
# å‡ºåŠ›ï¼šstorage/{date}_d.jsonï¼ˆschema_version=1.0ï¼‰
# æ—¢å®šã¯ã€ŒJSTã®ä»Šæ—¥ã€ã ã‘ã‚’æ›¸ãå‡ºã™ã€‚æ¤œè¨¼ç”¨ã«ç’°å¢ƒå¤‰æ•°ã§åˆ‡æ›¿å¯ã€‚
# å®Ÿè¡Œ: PS> python -m scrapers.congress_b

from __future__ import annotations
import os
import re
import json
import time
import hashlib
from typing import List, Dict, Optional, Tuple
from datetime import datetime, timezone, timedelta
from pathlib import Path

import requests
from requests.adapters import HTTPAdapter, Retry
from bs4 import BeautifulSoup

# ğŸ”¥ parser.py ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from utils.parser import split_and_normalize
except ImportError:
    # ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãƒ‘ã‚¹å•é¡Œã®å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    import sys
    sys.path.append(str(Path(__file__).resolve().parents[1]))
    from utils.parser import split_and_normalize

# ========= META =========
META = {
    "name": "congress_b",
    "venue": "ç¦å²¡å›½éš›ä¼šè­°å ´",
    "code": "d",
    "url_candidates": [
        "https://www.marinemesse.or.jp/congress/event/",
        #"https://www.marinemesse.or.jp/congress/schedule/",
        #"https://www.marinemesse.or.jp/congress/",
    ],
    "schema_version": "1.0",
    "selector_profile": "primary: table that has headers å„åˆ—ã€æ—¥æ™‚/ã‚¤ãƒ™ãƒ³ãƒˆå/ä¸»å‚¬è€…ã€ / alt: any table with similar header",
    "pagination": {
        "next_selector": "a[rel='next'], .pagination a",
        "max_pages": 5,
    },
}

# ========= SELECTORS =========
SELECTORS = {
    "primary_table_match": ("æ—¥æ™‚", "ã‚¤ãƒ™ãƒ³ãƒˆå", "ä¸»å‚¬è€…"),
    "alt_table_any": True,
}

# ========= ç’°å¢ƒãƒ»å…±é€š =========
JST = timezone(timedelta(hours=9))

def _split_and_normalize(s: str) -> str:
    if not s:
        return ""
    s = (
        s.replace(""", '"').replace(""", '"')
         .replace("'", "'").replace("'", "'")
         .replace("ã€œ", "ï½").replace("â€•", "ï¼")
    )
    s = re.sub(r"\s+", " ", s).strip()
    return s

def _sha1_hex(s: str) -> str:
    return hashlib.sha1(s.encode("utf-8")).hexdigest()

def _storage_path(date_str: str, code: str) -> Path:
    root = Path(__file__).resolve().parents[1]  # repo root (= event_notify/)
    storage = root / "storage"
    storage.mkdir(parents=True, exist_ok=True)
    return storage / f"{date_str}_{code}.json"

def _make_requests_session() -> requests.Session:
    s = requests.Session()
    s.headers.update({
        "User-Agent": "event_notify (congress_b) / maintainer: your-contact",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "ja,en;q=0.8",
        "Cache-Control": "no-cache",
        "Pragma": "no-cache",
        "Referer": "https://www.marinemesse.or.jp/",
        "Upgrade-Insecure-Requests": "1",
        "Connection": "keep-alive",
    })
    retries = Retry(
        total=3,
        backoff_factor=0.7,
        status_forcelist=(403, 429, 500, 502, 503, 504),
        allowed_methods=("GET",),
        raise_on_status=False,
    )
    s.mount("https://", HTTPAdapter(max_retries=retries))
    s.mount("http://", HTTPAdapter(max_retries=retries))
    return s

def _fetch_html(url: str, sess: requests.Session, timeout: float = 20.0) -> Optional[str]:
    try:
        r = sess.get(url, timeout=timeout)
        if r.status_code == 200 and "text/html" in r.headers.get("Content-Type", "") and r.text:
            return r.text
    except Exception:
        pass
    # fallbackï¼ˆä»»æ„ï¼‰
    try:
        time.sleep(1.2)
        import cloudscraper  # type: ignore
        scraper = cloudscraper.create_scraper(
            browser={"browser": "chrome", "platform": "windows", "mobile": False, "desktop": True},
            delay=5
        )
        scraper.headers.update(sess.headers)
        r2 = scraper.get(url, timeout=timeout + 5)
        if r2.status_code == 200 and "text/html" in r2.headers.get("Content-Type", "") and r2.text:
            return r2.text
    except Exception:
        pass
    return None

# ========= ãƒ‘ãƒ¼ã‚¹ =========
def _find_event_table(soup: BeautifulSoup) -> Optional[BeautifulSoup]:
    for t in soup.find_all("table"):
        header_text = _split_and_normalize(t.get_text(" ", strip=True))
        if all(key in header_text for key in SELECTORS["primary_table_match"]):
            return t
    tables = soup.find_all("table")
    return tables[0] if tables else None

def _parse_table(table: BeautifulSoup) -> List[Dict[str, str]]:
    events: List[Dict[str, str]] = []
    rows = table.find_all("tr")
    if not rows:
        return events
    for tr in rows[1:]:
        tds = tr.find_all(["td", "th"])
        if len(tds) < 2:
            continue
        when_raw = _split_and_normalize(tds[0].get_text(" ", strip=True))
        title_cell = tds[1]
        a = title_cell.find("a")
        title_raw = _split_and_normalize(a.get_text(" ", strip=True) if a else title_cell.get_text(" ", strip=True))
        link = a.get("href") if a and a.has_attr("href") else None
        if not when_raw or not title_raw:
            continue
        if link and link.startswith("/"):
            link = "https://www.marinemesse.or.jp" + link
        events.append({"when": when_raw, "title": title_raw, "link": link or META["url_candidates"][0]})
    return events

# ğŸ”¥ æ–°ã—ã„ _materialize_events - parser.py ã‚’ä½¿ç”¨
def _materialize_events(rows: List[Dict[str, str]]) -> List[Dict]:
    base_year = datetime.now(JST).year
    out: List[Dict] = []
    
    for r in rows:
        when = r["when"]
        title = r["title"]
        source = r["link"]
        
        # ğŸ¯ parser.py ã® split_and_normalize ã‚’ä½¿ç”¨
        parsed_events = split_and_normalize(when, title, META["venue"], base_year)
        
        for ev in parsed_events:
            item = {
                "schema_version": META["schema_version"],
                "date": ev["date"],
                "title": ev["title"],
                "venue": ev["venue"],
                "source": source,
            }
            
            # æ™‚åˆ»ãŒã‚ã‚Œã°è¿½åŠ 
            if ev.get("time"):
                item["time"] = ev["time"]
            else:
                # æ™‚åˆ»æœªå®šã®å ´åˆã€å…ƒã®æ–‡å­—åˆ—ã‚’notesã«ä¿å­˜
                item["notes"] = when
                
            out.append(item)
    
    return out

def _dedupe_and_hash(items: List[Dict]) -> List[Dict]:
    seen = set()
    norm_items: List[Dict] = []
    for ev in items:
        date = ev["date"]
        time_s = ev.get("time", "")
        title_norm = _split_and_normalize(ev["title"])
        venue_norm = _split_and_normalize(ev["venue"])
        key = f"{date}|{time_s}|{title_norm}|{venue_norm}"
        h = _sha1_hex(key)
        if h in seen:
            continue
        seen.add(h)
        ev["hash"] = h
        ev["extracted_at"] = datetime.now(JST).isoformat(timespec="seconds")
        norm_items.append(ev)
    def sort_key(e: Dict) -> Tuple:
        return (e["date"], e.get("time", "99:99"), _split_and_normalize(e["title"]))
    return sorted(norm_items, key=sort_key)

# ========= ä»Šæ—¥æŠ½å‡ºï¼ˆæ–°è¦ï¼‰ =========
def _resolve_target_date() -> str:
    """SCRAPER_TARGET_DATE=YYYY-MM-DD ãŒã‚ã‚Œã°ãã‚Œã‚’å„ªå…ˆã€‚ãªã‘ã‚Œã°JSTã®ä»Šæ—¥ã€‚"""
    override = os.getenv("SCRAPER_TARGET_DATE")
    if override:
        return override
    return datetime.now(JST).strftime("%Y-%m-%d")

def _filter_today_only(items: List[Dict], target_date: str) -> List[Dict]:
    return [e for e in items if e.get("date") == target_date]

# ========= ãƒ¡ã‚¤ãƒ³ =========
def scrape_once(url: str, sess: requests.Session) -> List[Dict]:
    html = _fetch_html(url, sess)
    if not html:
        return []
    soup = BeautifulSoup(html, "html.parser")
    table = _find_event_table(soup)
    if not table:
        return []
    rows = _parse_table(table)
    items = _materialize_events(rows)
    return _dedupe_and_hash(items)

def main():
    t0 = time.time()
    sess = _make_requests_session()
    tried_urls: List[str] = []

    # 1) åé›†ï¼ˆå€™è£œURLã‚’é †ã«ï¼‰
    collected: List[Dict] = []
    for url in META["url_candidates"]:
        tried_urls.append(url)
        try:
            items = scrape_once(url, sess)
        except Exception as e:
            print(f"[{META['name']}][ERROR] msg=\"{e}\" url=\"{url}\"")
            items = []
        if items:
            collected = items
            break
        time.sleep(1.2)  # polite

    # åé›†ã™ã‚‰ã‚¼ãƒ­ï¼ˆHTMLå–ã‚Œãªã„/ãƒ†ãƒ¼ãƒ–ãƒ«è¦‹ã¤ã‹ã‚‰ãªã„ç­‰ï¼‰ã¯"å¤±æ•—æ‰±ã„"ã§éç”Ÿæˆ
    if collected == [] and len(tried_urls) == len(META["url_candidates"]):
        elapsed_ms = int((time.time() - t0) * 1000)
        print(f"[{META['name']}][ERROR] msg=\"no events parsed\" tried={len(tried_urls)} ms={elapsed_ms}")
        return

    # 2) ä»Šæ—¥æŠ½å‡ºï¼ˆæ—¢å®šï¼‰ï¼å…¨é‡ä¿å­˜ï¼ˆãƒ•ãƒ©ã‚°ï¼‰
    target_date = _resolve_target_date()
    include_future = os.getenv("SCRAPER_INCLUDE_FUTURE") == "1"
    items_to_save = collected if include_future else _filter_today_only(collected, target_date)

    # 3) ä¿å­˜ï¼ˆå½“æ—¥0ä»¶ã§ã‚‚ç©ºé…åˆ—ã‚’æ›¸ãå‡ºã™ï¼šç›£è¦–ã®éƒ½åˆã§æˆåŠŸæ‰±ã„ï¼‰
    outpath = _storage_path(target_date, META["code"])
    with outpath.open("w", encoding="utf-8") as f:
        json.dump(items_to_save, f, ensure_ascii=False, indent=2)

    # 4) ãƒ­ã‚°
    elapsed_ms = int((time.time() - t0) * 1000)
    print(f"[{META['name']}] date={target_date} items={len(items_to_save)} ms={elapsed_ms} include_future={int(include_future)}")

if __name__ == "__main__":
    main()
